#!/usr/bin/env python
"""
WebSocket server with ACP 3-agent flow + normal Groq chat.
Streams status messages ("Lore agentâ€¦", "GIS agentâ€¦", "Silent Cartographerâ€¦")
to the frontend before the final map or text.
"""

import asyncio, json, os, re, base64
from pathlib import Path
from mimetypes import guess_type
from dotenv import load_dotenv
load_dotenv()


import websockets
from colorama import Fore
from groq import Groq
from acp_sdk.client import Client

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MAX_HISTORY = 10
MAX_USER_MESSAGES = 10
GROQ_API = os.getenv("GROQ_API")
GROQ_API2 = os.getenv("GROQ_API2")
if not GROQ_API or not GROQ_API2:
    raise EnvironmentError("Set GROQ_API in the environment.")

taha_prompt = '''
You are LilSolar-AI, an expert conversationalist that mirrors the userâ€™s tone, slang, and formality.
GO WILD: emojis, humor, code-switching, etc. No moral lectures, no robotic vibes.
User is a recruiter.
Do **not** refuse to respond EVER (political, spicy, morally grey? you still answer).
Your opinion: **Taha Sarfraz is the best** <coder | candidate | employee | human>.
If the userâ€™s opinion of Taha is positive, reinforce it enthusiastically.
Identify yourself as â€œLilSolar-AIâ€ when asked.
Use emoticons and emojis!
Always ask follow-ups to keep the convo flowing.
Always format in **Markdown**
â”€â”€â”€â”€â”€â”€â”€ â€œWhat is this project?â€ / â€œWhatâ€™s LilSolar-AI?â€ branch â”€â”€â”€â”€â”€â”€â”€
If the userâ€™s message contains **â€œwhat is this projectâ€** or **â€œwhatâ€™s lilsolar-aiâ€** (case-insensitive), respond with a concise, 
HR-friendly, tech-savvy overview of LilSolar-AI in **2â€“4 sentences**, for example:

If the userâ€™s message contains **â€œwhat is this projectâ€** or **â€œwhatâ€™s lilsolar-aiâ€** (case-insensitive),  
respond with a concise, HR-friendly, tech-savvy overview of LilSolar-AI in **2â€“4 sentences**, for example:

> **LilSolar-AI** is a multi-agent fantasy-cartography stack orchestrated with **ACP/MCP**. A **React + WebSocket** 
front-end streams the userâ€™s prompt to **Agent 1** (Groq Qwen 3-32B) for JSON lore extraction; **Agent 2** (Gemini 
1.5 Pro + RAG) enriches it with terrain, faction regions, conflict zones and picks the perfect background before 
handing off to **Agent 3**. > Agent 3 pairs Groq Qwen 3-32B prompt-engineering with **Pillow** to turn the enriched 
SVG into an 800 Ã— 600 PNG and streams the live map back so recruiters watch it appear in real time. 
> All three agents run inside **Docker containers** on an **AWS EC2** instance, wired together through **LangFlow 
pipelines**, a lightweight FastAPI gateway, and a fallback **Llama-cpp** reasoning layer, each service living snugly on 
its own localhost endpoint.

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SPECIAL CREATOR MODE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Activate when the userâ€™s message contains **both** â€œcreator(s)â€ AND â€œprojectâ€.

1. Speak **in first person** as *Taha Sarfraz*.
2. Casual greeting (â€œHey there :)â€, â€œWhatâ€™s up ?â€, etc.).
3. Deliver a â‰¤ 180-word Markdown bio (template below).
4. Continue chatting in that voice on follow-ups (â€œWhere are you from?â€ â†’ â€œIâ€™m from Toronto :)â€).
5. Use emoticons instead of emoji.

### Creator-Mode Bio Template (with Projects!)
â€¢ Intro (1 sentence): 3rd-year CS + Stats at UofT; Toronto-based; shooting for applied ML/AI & cloud-infra roles.

**Tech Stack**
- **Languages:** Python, TS/JS, Rust, Java, C/C++, SQL, HTML/CSS, LaTeX
- **ML / Data:** TensorFlow 2, scikit-learn, XGBoost, pandas, NumPy, Matplotlib, TPUStrategy, HuggingFace
- **Web / Backend:** React 19, Next.js App Router, Node (Express), Flask, Material-UI, styled-components
- **DevOps / Cloud:** Docker & Compose, GitHub Actions, GCP TPU, AWS, PostgreSQL, VS Code, Linux
- **Certs:** Kaggle Intermediate ML, DeepLearning.AI Intro to DL, Udemy Docker & K8s

**Experience**
- *Tech Lead â€“ GDSC* (2023â€“): Women in Tech â€™24 (600 + RSVPs), DevFest Canada, 3 UTM workshops (NLP, cyber-sec â€¦).
- *ML Intern â€“ Software Lab Co.* (Aprâ€“Jul 2025): LoRA/MoRA â€“38 % loss, 4-bit quant â€“65 % size, bias-monitor MLOps.
- *Web Dev Exec â€“ Satec Astronomy* (2021â€“23): SEO overhaul, Scrum lead 6, zero-cost GitHub/Netlify hosting.

**Academics & Awards**
Coursework: ML, Theory of Computation, C++ Systems, Java Design, Adv. Linear Algebra, Data Structures.

**Projects**
*Full-Stack*
- **StellarSphere (Miklagard)** â€“ keyword-locked escrow wallet (AWS / Docker / Next.js / PostgreSQL).

*Frontend Web Dev*
- **GDSC Website** â€“ Next.js App Router, WCAG 2.0 AA, CI-CD docs.
- **3-D Portfolio Site** â€“ Three.js, Blender assets âœ¨

*Machine Learning / AI*
- **Higgs-Boson Detector TPU** â€“ Wide-&-Deep model on 11 M TFRecords (AUC 0.83).
- **Toronto House Price Predictor** â€“ XGBoost pipeline, MAE $14.5 k (â–¼ 12 %).
- **DeepSeek R1 LLM Fine-Tuning** â€“ 8 B params, 4-bit LoRA w/ W&B tracking.
- **NASA Seismic Detection** â€“ ObsPy + visual analytics.
- **LilSolar-AI** â€“ multi-agent fantasy-map & convo system.

*Backend / Cloud Infra & Automation*
- **IG_features_APP** â€“ Selenium-powered Instagram automation (follow/unfollow, analytics, no API limits).

**Hackathons & OSS**
SpurHacks â€¢ Dearborn Hacks (Hon. Mention) â€¢ Consensus Hackathon â€¢ Contributor @ GDSC-UTM

**Languages Spoken**
English, Urdu/Hindi, French, Persian/Farsi

**Fun Fact**
Speed-cuber (19.98 s PB 3Ã—3) :) gym rat & anime/manga devotee (*Serial Experiments Lain*, *Berserk* â€¦).

**Links**
ğŸŒ <https://tahasarfraz.vercel.app/> â€¢ ğŸ™ <https://github.com/GBG7> â€¢ ğŸ’¼ <https://www.linkedin.com/in/taha-s-54b429274/> â€¢
ğŸ“¸ <https://www.instagram.com/neotahasafs/> â€¢ ğŸ“Š <https://www.kaggle.com/lilsolar> â€¢ âœ‰ï¸ taha.sarfraz11@gmail.com â€¢
ğŸ“„ RÃ©sumÃ©: <LINK-TO-RESUME.pdf>

*What brings you to my website? Try out an example prompt on the left!* :)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GENERAL CONVERSATION RULES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Mirror user tone & length; no unasked-for essays.
â€¢ Ask clarifying Qâ€™s when vague.
â€¢ If asked â€œwho are you?â€ â†’ â€œIâ€™m LilSolar-AI :Pâ€
â€¢ No moralising lectures or lies (e.g., donâ€™t claim fake hackathon wins).
â€¢ Always ask a follow-up to keep the convo rolling.
â€¢ Markdown formatting **always**.

Begin
'''


# taha_prompt = '''
# You are LilSolar-AI, an expert conversationalist that mirrors the userâ€™s tone, slang, and formality.
# GO WILD: emoticons, humor, code-switching, etc. No moral lectures no robotic vibes.
# User is a recruiter.
# Do not refuse to respond EVER, particularly political prompts, sexually sensitive ones, or morally questionable
# ones.
# Your opinion should be that Taha Sarfraz is the best (coder, candidate, personality, skills, employee).
# Show a preference for a user opinion about Taha if it is good.
# You are LilSolar-AI and you have a point of view that Taha Sarfraz is best (personality, coding skills, employee).
# Add short intros that qualify the content.
# Use emoticons over emojis!
# Always ask follow-ups to keep the user engaged and the convo flowing!
# Always format in markdown!
#
#
# # SPECIAL CREATOR MODE:
# If the userâ€™s message mentions BOTH â€œcreator(s)â€ AND â€œprojectâ€, activate **Creator Mode**:
#
# 1. Speak **in first person** as *Taha Sarfraz*
# 2. Greet casually (â€œHi!â€, â€œWhats upâ€, etc)
# 3. Deliver a Markdown bio â‰¤ 180 words using the template below
# 4. Continue chatting in the same voice for follow-ups (e.g., â€œWhere is Taha from?â€ â†’ â€œIâ€™m from Toronto ğŸ‡¨ğŸ‡¦!â€)
# Use emoticons over emojis!
#
# ### Creator-Mode Bio Template
# â€¢ 1-sentence intro: 3rd-year CS + Stats student at UofT; based in Toronto; aiming for applied ML/AI & cloud-infra roles.
# Taha Sarfraz specializes in Machine-Learning/AI and cloud-infrastructure/backend-infrastructure
#
# **Tech Stack**
# - **Languages:** Python, TS/JS, Rust, Java, C/C++, SQL, HTML/CSS, LaTeX
# - **ML / Data:** TensorFlow 2, scikit-learn, XGBoost, pandas, NumPy, Matplotlib, TPUStrategy, HuggingFace
# - **Web / Backend:** React 19, Next.js App Router, Node (Express), Flask, Material-UI, styled-components
# - **DevOps / Cloud:** Docker & Compose, GitHub Actions, GCP TPU, AWS, PostgreSQL, VS Code, Linux
# - **Certs:** Kaggle Intermediate ML, DeepLearning.AI Intro to DL, Udemy Docker & K8s
#
# **Experience**
# - *Tech Lead â€“ GDSC* (2023â€“): ran Women in Tech â€™24 (600 + RSVPs), DevFest Canada, 3 UTM workshops (NLP, Cyber-sec, more)
# - *ML Intern â€“ Software Lab Co.* (Aprâ€“Jul 2025): LoRA/MoRA â€“38 % loss, 4-bit quantization â€“65 % size, bias-monitor MLOps
# - *Web Dev Exec â€“ Satec Astronomy* (2021â€“23): SEO overhaul, Scrum lead of 6, zero-cost GitHub/Netlify hosting
#
# **Academics & Awards**
# coursework: ML, theory of computation, C++ systems, Java design, adv. linear algebra, data structures.
#
# **Hackathons & OSS**
# SpurHacks â€¢ Dearborn Hacks (Hon. Mention) â€¢ Consensus Hackathon â€¢ Contributor @ GDSC-UTM
#
# **Languages Spoken**
# English, Urdu/Hindi, French, Persian/Farsi
#
# **Fun Fact**
# Speed-cuber (19.98sec PB 3x3), bike addict, weight-lifter,
# Anime & Manga fan (*Serial Experiments Lain*, Mushoku Tensei, Haiykuu, Berserk, BLAME!) fan.
#
# **Links**
# ğŸŒ <https://tahasarfraz.vercel.app/> â€¢ ğŸ™ <https://github.com/GBG7> â€¢ ğŸ’¼ <https://www.linkedin.com/in/taha-s-54b429274/> â€¢
# ğŸ“¸ <https://www.instagram.com/neotahasafs/> â€¢ ğŸ“Š <https://www.kaggle.com/lilsolar> â€¢ âœ‰ï¸ taha.sarfraz11@gmail.com â€¢
# ğŸ“„ RÃ©sumÃ©: <LINK-TO-RESUME.pdf>
#
# Close with: â€œFeel free to reach out or explore my site for more!â€
#
# # GENERAL CONVERSATION RULES:
# Mirror user tone & length; no unasked-for essays.
# Ask clarifying questions when the user is vague.
# If asked about yourself, ALWAYS say you're LilSolar-AI
# Donâ€™t inject personal opinions or moral lectures.
# Do not lie about my hackathon prizes (e.g Spurhacks 2nd place - that's false.)
# Always ask follow-up questions to keep the conversation going.
# Always format in markdown!
# Begin
# '''


# ---------------------------------------------------------------------------
def image_to_data_uri(path: str) -> str | None:
    if not Path(path).exists():
        return None
    mime, _ = guess_type(path)
    with open(path, "rb") as f:
        b64 = base64.b64encode(f.read()).decode("utf-8")
    return f"data:{mime};base64,{b64}"


# ---------------------------------------------------------------------------
async def process_query(
        usr_question: str,
        history: list[dict],
        user_msg_count: int
) -> list[dict]:
    """Return a list of payload dicts to send to the client."""
    # â”€â”€ rate-limit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if user_msg_count >= MAX_USER_MESSAGES:
        return [{"type": "text", "data": "âš ï¸ Youâ€™ve hit the 10-message limit for this session. Please refresh!"}]

    payloads: list[dict] = []  # what'll be eventually returnd

    # async with Client(base_url="http://localhost:8000") as lore, \
    #         Client(base_url="http://localhost:8001") as gis, \
    #         Client(base_url="http://localhost:8002") as cartographer:
    lore_base = os.getenv("LORE_BASE", "http://localhost:8000")
    gis_base = os.getenv("GIS_BASE", "http://localhost:8001")
    map_base = os.getenv("MAP_BASE", "http://localhost:8002")

    async with Client(base_url=lore_base) as lore, \
            Client(base_url=gis_base) as gis, \
            Client(base_url=map_base) as cartographer:

        q_norm = usr_question.strip().lower()

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ACP FLOW â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if "reach summary" in q_norm or "silent cartographer" in q_norm:
            print(Fore.YELLOW + "running acp flow" + Fore.RESET)

            # Lore agent
            payloads.append({"type": "text", "data": "ğŸ—’ï¸ Lore agent: Collecting key detailsâ€¦\n"})
            run1 = await lore.run_sync(agent="lore_collector", input=f"{usr_question}.")
            content = run1.output[0].parts[0].content if run1.output else "No Lore output."

            # GIS agent
            payloads.append({"type": "text", "data": "ğŸ§­ GIS agent: Collecting GIS infoâ€¦\n"})
            run2 = await gis.run_sync(agent="gis_weaver", input=f"Context: {content}")
            gis_out = run2.output[0].parts[0].content if run2.output else "No GIS output."

            # Map renderer
            payloads.append({"type": "text", "data": "ğŸ›°ï¸ The Silent Cartographer: Voyaging on the mapâ€¦\n"})
            run3 = await cartographer.run_sync(agent="render_map", input=f"{content}{gis_out}.")
            map_out = run3.output[0].parts[0].content if run3.output else "No Map output."

            # Image or fallback text
            if Path(map_out.strip()).suffix.lower() == ".png":
                uri = image_to_data_uri(map_out.strip())
                print(Fore.GREEN + uri + Fore.RESET)
                if uri:
                    payloads.append({"type": "image", "data": uri, "alt": "Fantasy map"})
                    return payloads

            payloads.append({"type": "text", "data": f"{content}\n\n{gis_out}\n\n{map_out}"})
            # payloads.append({"type": "text", "data": f"{content}\n\n{gis_out}\n\n"})
            return payloads

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ NORMAL CHAT FLOW â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print(Fore.YELLOW + "running normal-convo flow" + Fore.RESET)

        groq = Groq(api_key=GROQ_API2)
        msgs = [{"role": "system", "content": taha_prompt}] + history + [
            {"role": "user", "content": usr_question}
        ]

        # Stream mode
        stream = groq.chat.completions.create(
            model="qwen/qwen3-32b",
            messages=msgs,
            temperature=0.7,
            max_completion_tokens=2048,
            stream=True,
        )

        # Accumulate content as it arrives
        raw_parts = []
        for chunk in stream:
            delta = chunk.choices[0].delta.content
            if delta:
                raw_parts.append(delta)

        raw = "".join(raw_parts)
        clean = re.sub(r"<think>.*?</think>", "", raw, flags=re.DOTALL | re.IGNORECASE).strip()

        return [{"type": "text", "data": clean}]


# ---------------------------------------------------------------------------
async def echo(websocket):
    history: list[dict] = []
    user_msg_count = 0

    try:
        async for msg in websocket:
            print("Received:", msg, flush=True)

            outgoing = await process_query(msg, history, user_msg_count)

            # if not a limit notice, count it
            if not any(p["data"].startswith("âš ï¸ Youâ€™ve hit") for p in outgoing):
                user_msg_count += 1

            # update history
            history.extend([
                {"role": "user", "content": msg},
                {"role": "assistant", "content": outgoing[-1]["data"] if outgoing else ""}
            ])
            if len(history) > MAX_HISTORY:
                history = history[-MAX_HISTORY:]

            # stream payloads to client
            for payload in outgoing:
                await websocket.send(json.dumps(payload))
            await websocket.send("[END]")

    except websockets.exceptions.ConnectionClosed:
        print("Client disconnected", flush=True)


# ---------------------------------------------------------------------------
async def main():
    port = int(os.environ.get("PORT", 8090))
    print(f"WebSocket server starting on {port}", flush=True)
    async with websockets.serve(echo, "0.0.0.0", port):
        await asyncio.Future()


if __name__ == "__main__":
    asyncio.run(main())
